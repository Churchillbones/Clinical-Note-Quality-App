# Clinical Note Quality Grader – Project Overview

## Vision

Enable VA clinicians and informatics teams to **rapidly, objectively, and reproducibly** assess the quality of any clinical EHR note—especially those autogenerated by large‑language‑model scribes—thereby accelerating safe adoption of ambient documentation tools.

## Mission

Build an open‑source, Azure‑native micro‑service that:

1. Accepts a clinician note (and optional transcript).
2. Scores it on established documentation rubrics (PDQI‑9 plus hybrid factuality/heuristics).
3. Surfaces clear, actionable feedback and an overall quality score within seconds.
4. Operates entirely inside VA‑approved infrastructure with no PHI leakage.

## High‑Level Goals

| #  | Goal                         | Success Metric                                               | Target Timeline |
| -- | ---------------------------- | ------------------------------------------------------------ | --------------- |
| G1 | **Automated PDQI‑9 Scoring** | O3‑based scores match expert raters with ICC ≥ 0.80          | 60 days         |
| G2 | **Hybrid Quality Composite** | Composite ρ > 0.7 vs. human "overall quality" Likert ratings | 75 days         |
| G3 | **Sub‑10‑Second Latency**    | p95 grading time ≤ 10 s for <10 KB notes                     | 45 days         |
| G4 | **Cost Efficiency**          | ≤ \$0.005 per note at 2025 O3 pricing                        | 60 days         |
| G5 | **VA Compliance**            | Full security review & ATO fast‑track                        | 90 days         |

## SMART Objectives

1. **Design & Implement** a Flask API with `/api/grade` endpoint returning PDQI‑9 JSON by **Week 2**.
2. **Integrate Azure OpenAI O3** via function‑calling prompt for rubric scoring by **Week 3**.
3. **Add heuristic modules** (redundancy, length, Flesch‑Kincaid, factual entailment) and merge into hybrid score by **Week 4**.
4. **Develop unit test suite** (pytest, 95 % coverage, mocked external calls) by **Week 5**.
5. **Deploy DEV instance** to OpenShift with HPA and sealed‑secrets by **Week 6**.
6. **Collect 100 sample notes** across 3 specialties and benchmark ICC vs. two clinician raters by **Week 8**.
7. **Iterate prompts/weights** until ICC ≥ 0.80; lock v1.0 scorecard by **Week 9**.
8. **Document full CI/CD** pipeline, README, and Jupyter batch‑evaluation demo by **Week 10**.
9. **Submit security package & ATO request** (SOC template, threat model) by **Week 11**.

## Future Extensions

* **Section‑level QNOTE scoring** for granular feedback.
* **Explainability**: highlight transcript snippets supporting each score.
* **Fine‑tuned O3 model** trained on VA documentation examples.
* **Integration hooks** for Epic CPRS or PowerApps dashboard.

---

*Last updated: {{TODAY}}.*
